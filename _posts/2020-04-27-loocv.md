---
title: "Comparing Dependent Variable Model Transformations by Leave-one-out Cross-Validation in R"
date: 2020-04-27
categories:
  - Post
excerpt: "Optimising the predictive power of linear regression models after transforming the dependent variable by utilising cross-validation."
---

When deciding on a dependent variable transformation to enable optimal predictive power, a key concern is that the R-squared values (which are commonly used to measure the predictive fit of different models) are no longer comparable. Since R-squared is the proportion of the variance in the dependent variable that is explained by the variation in the independent variables, by performing transformations on the dependent variable we have also transformed their variance structures. As such, we can no longer gain insight into the relative predictive power of the different models by comparing their R-squared values.

Box-Cox Power Transformations
Although one may use the Box-Cox power transformation to get an indication of which transformation will help our model fit the data, this method will not help us with a measure of how well the transformed model performs for prediction, nor will it indicate whether using the mean or median predictor when rewinding the transformation will yield the best predictions. The following example highlights the issue relying too heavily on the Box-Cox transformation alone by employing leave-one-out cross-validation to calculate a comparable predictive power measure.

Leave-one-out Cross-validation

Cross-validation can be used as a means to enable comparison of the different dependent variable transformations. The leave-one-out method of cross-validation uses one observation from the sample data set to be used as the validation data, using the remaining observations as training data. Each model is used to predict the value of the validation data point by fitting the training data, then a prediction error is calculated by subtracting the predicted value from the validation data observation.
To ensure we obtain comparable measures, it is essential that the fitted value produced by the transformed models are converted back to the “level” form to obtain a predicted value that is in the same units as the original observation data. Using different methods for this conversion enables the comparison of the mean and median predictors. This process is repeated, until each observation from the original sample has been used as a validation data point, then by summing the absolute values* of the prediction errors we obtain a measure of how well the transformed model in question makes out-of-sample predictions.
Once the cross-validation measure has been calculated for each transformation (and applicable predictor type), the model that produces the lowest measure is deemed to have the greatest predictive power.

'* One may instead wish to use the sum of squares of the prediction errors to calculate the comparison.
